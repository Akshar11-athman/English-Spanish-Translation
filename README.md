English-to-Spanish Translation using Deep Learning

ðŸ”¹ Project Description

This project focuses on Neural Machine Translation (NMT) from English to Spanish.
We implemented and compared classical recurrent models (RNN, LSTM, GRU) with a modern Transformer-based model (BERT / MarianMT).

The main goal is to understand how traditional sequence models differ from advanced pretrained Transformers in terms of translation accuracy.

ðŸ”¹ Dataset

Source: Kaggle Englishâ€“Spanish parallel dataset

Size: ~20,000 sentence pairs (subset used for training due to compute limits)

Preprocessing:

Text cleaning

Tokenization (word â†’ integer IDs)

Padding to fixed sequence length (20 tokens)

Train/Test split (80/20)

ðŸ”¹ Models Implemented

RNN â€“ Simple sequential model

LSTM â€“ Handles long-term dependencies

GRU â€“ Lightweight LSTM alternative

BERT Transformer (MarianMT) â€“ Pretrained state-of-the-art translation model

ðŸ”¹ Training Setup

Frameworks: TensorFlow/Keras, HuggingFace Transformers

Optimizer: Adam

Loss: Sparse Categorical Cross-Entropy

Epochs: 3

Batch size: 64

ðŸ”¹ Results

Evaluation Metric: BLEU Score

Model	BLEU Score
RNN	20.56
LSTM	17.29
GRU	9.29
BERT (MarianMT)	41.11

ðŸ“Š BERT significantly outperforms recurrent models.

ðŸ”¹ Example Translations

EN: Hello â†’ ES: Hola

EN: Good morning â†’ ES: Buenos dÃ­as

EN: What is your name? â†’ ES: Â¿CÃ³mo te llamas?

EN: How are you?

Ground Truth: Â¿CÃ³mo estÃ¡s?

Predicted (GRU): ...

Predicted (BERT): Â¿CÃ³mo estÃ¡s?

ðŸ”¹ Future Work

Train on larger datasets

Apply beam search decoding

Use multilingual pretrained models (mBART, mT5)

Deploy as a web app or API

ðŸ”¹ References

Kaggle English-Spanish Dataset

HuggingFace MarianMT

TensorFlow/Keras

SacreBLEU Toolkit
